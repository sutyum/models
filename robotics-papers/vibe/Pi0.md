# Quick Screen: Pi0 - A vision-language-action flow model for general robot control

**Authors / Venue / Year:** Kevin Black, ..., <u>Chelsea Finn</u>, ..., <u>Sergey Levine</u>, ..., Ury Zhilinsky / Arxiv / Oct 24
**Link:** https://arxiv.org/pdf/2410.24164

---

## 30-Second Skim (Abstract + Figures)

**What's the claim?** (One line)
Introduces flow matching architecture for building VLAs - in order to achieve higher generalization over methods like OpenVLA and RT-2.

**The figure that tells the story:** (Figure #)
Figure 1/3. Both show the basic architecture, embodiments and dataset composition.


**Task domain:** [x] Manipulation  [ ] Locomotion  [ ] Navigation  [x] Multi-task  [ ] Other: ___

---

## Relevance Filter

| Relevant to... | Yes/No | How? |
|----------------|--------|------|
| VLA / embodied AI | Yes | Open wight VLA with use of some proprietary data |
| Motor control / low-level |  |  |
| Sim2Real |  |  |
| Data efficiency |  |  |
| My current problem: ___ |  |  |

---

## Quality Signals (Flip through)

- [x] Real robot results (not just sim)
- [ ] Ablations present
- [x] Code released
- [x] Comparisons to recent (<2 yr) baselines
- [x] Clear failure cases shown

**Red flags spotted:**

---

## Gut Check

**Novelty:** [ ] Incremental  [x] Solid contribution  [ ] Potentially big idea

**Could I explain the core idea right now?** [ ] Yes  [x] Vaguely  [ ] No (complexity signal)

**One thing that made me curious:**
Use of flow matching. Why is this useful enough?

---

## Verdict

[x] **DEEP READ** - High relevance, worth 2+ hours
[ ] **SKIM** - Grab the technique, skip details  
[ ] **ARCHIVE** - Reference later if needed
[ ] **SKIP** - Not for me right now

**If reading, focus on Section(s):**
